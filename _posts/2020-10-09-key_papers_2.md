# Key Papers in RL #2: Deep Recurrent Q-Learning for Partially Observable MDPs

The second paper in the Model-free series of papers looks at what happens when a Deep-Q Network is presented with a problem: what happens when you can't see everything? This is the question posed by the paper's authors along with a solution.


## Paper Basics

The second paper in this series is a 2015 paper [Deep Recurrent Q-Learning for Partially Observable MDPs](https://arxiv.org/abs/1507.06527) by Matthew Hausknecht and Peter Stone, both of UT Austin. This is a shorter paper than the first in the series, but it certainly adds some wrinkles to the RL space.

Before delving too far into the paper, it helps to remember what advantages Deep-Q Networks have when looking at the state of a game, in this case an Atari game. Atari games are fully described by the 128 bytes of memory within the Atari console and game cartridge (or ROM in our case). This means that nothing is hidden from the agent playing these games. While this is helpful with regards to Atari games, it is rarely the case that the problem space will be fully observable. Most situations in robotics, for instance, will have states that are not fully observable. The same situation is present in self-driving vehicles; hazards are often not in range of sensors or obscured by other vehicles which can lead to false negatives.

In order to test the effect of obscuring the game screen, the authors simply fully revealed or fully obscured each timestep's frame with a probability $p=0.5$. The authors used DQN as a baseline to investigate how the algorithm would generalize from a Markov Decision Process to a partially observed MDP. In this case DQN's performance degraded quite badly in the POMDP scenario compared to the normal MDP.

## Big Idea

The big idea of this paper is to use most of the network architecture used in a standard DQN and substitute a Long Short Term Memory (LSTM) layer in place of the final fully connected layer. The addition of the recurrent layer makes the DQN a Deep Recurrent Network or DRQN. The results of using this new architecture are that the agent will perform substantially better in the POMDP scenario than a regular DQN while being roughly equivalent in performance in the standard MDP scenario.

## Final Thoughts

This paper was really interesting to read; I had not considered the possibility of incomplete environment information as most of the Gym environments are Atari or similar level of complexity. I am curious to see if techniques like this would work in games where a "fog of war" or similar information hiding strategy is present.
